#!python37

## By Elim Thompson (09/05/2020)
##
## This script compares different sets of statistics generated by different
## settings / configuration. This script accepts up to 3 sets. Each set has
## its own processed folder where train_stats.csv, valid_stats.csv, and
## test_stats.csv are available. 
##
## > python compare_sets.py --out_path <Path to store output plots>
##                          --set1_path <Path to processed data of set 1>
##                         (--set2_path <Path to processed data of set 2>)
##                         (--set3_path <Path to processed data of set 3>)
###############################################################################

###############################################
## Import libraries
###############################################
import logging, pandas, numpy, argparse, os
import data_cleaner

import matplotlib
matplotlib.use ('Agg')
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
plt.rc ('text', usetex=False)
plt.rc ('font', family='sans-serif')
plt.rc ('font', serif='Computer Modern Roman')

###############################################
## Define constants
###############################################
# Default output path
out_path = 'C:/Users/lindsay.abrams/Documents/noaa-wl-ai/data/'

# Default set 1 path
set1_path = 'C:/Users/lindsay.abrams/Documents/noaa-wl-ai/processed/'

# Plot styles - Up to 3 sets max
colors  = ['black', 'blue', 'red']
markers = ['x', '^', 'o']

# Percetage threshold - 10%
PCT_THRESH = 0.1

# Number of randomly sampled data in Greg's AI model when training
N_RANDOM_SAMPLES = 200000

###############################################
## Define functions
###############################################
def check_if_path_exists (apath):

    ''' A private function to check if an input path exists. If it doesn't, a
        FileNotFoundError is raised.
        
        input param
        -----------
        apath (str): A folder to be checked
    '''

    if not os.path.exists (apath):
        message = 'Path or file, {0}, does not exist!'.format (apath)
        raise FileNotFoundError (message)    

def check_complete_set (apath):

    ''' A function to check if a give set is complete. It must fulfills the
        following requirements:
            1. The path itself must exist
            2. In the folder, a train_stats.csv exists
            3. In the folder, a valid_stats.csv exists
            4. In the folder, a test_stats.csv exists
        If any of the above is invalid, an exception is raised.
    '''

    ## Make sure the folder exists
    check_if_path_exists (apath)
    ## 1. train_stats.csv must exist
    check_if_path_exists (apath + '/train_stats.csv')
    ## 2. valid_stats.csv must exist
    check_if_path_exists (apath + '/valid_stats.csv')
    ## 3. test_stats.csv must exist
    check_if_path_exists (apath + '/test_stats.csv')

def get_parser ():
    
    ''' A function to handle user inputs via command line. Each flag is a path
        to a set of processed data. In each folder, there must exist a train_
        stats.csv, valid_stats.csv, and test_stats.csv. If either of them is
        missing, an exception is raised. An output path is also required to
        store output summary stats plot.

        return params
        -------------
        out_path  (str): Path to store output plot
        set1_path (str): Path to processed data from set 1
        set2_path (str): Path to processed data from set 2
        set3_path (str): Path to processed data from set 3
    '''

    ## Define parser to get arguments
    parser = argparse.ArgumentParser (description='')
    parser.add_argument('-o', '--out_path', default=out_path, type=str,
                        help='Path where output plots are stored')
    parser.add_argument('-a', '--set1_path', default=set1_path, type=str,
                        help='Path where set 1 cleaned data are stored')
    parser.add_argument('-b', '--set2_path', default=None, type=str,
                        help='Path where set 1 cleaned data are stored')
    parser.add_argument('-c', '--set3_path', default=None, type=str,
                        help='Path where set 1 cleaned data are stored')                                                
    args = parser.parse_args()

    ## 1. output path must exist
    check_if_path_exists (args.out_path)

    ## 2. Set1 must exist (at least 1 set of data to plot)
    check_complete_set (args.set1_path)

    ## 3. If set 2 is available, check it too!
    if args.set2_path is not None: check_complete_set (args.set2_path)

    ## 4. If set 3 is available, check it too!
    if args.set3_path is not None: check_complete_set (args.set3_path)

    return args.out_path, args.set1_path, args.set2_path, args.set3_path

def get_a_stats (dtype, set_path):
    
    ''' A function to read and re-format a stats csv file from input path.
            1. Index of stats dataframe is set to be station ID 
            2. Rename columns based on the child folder

        input params
        ------------
        dtype (str): either train, valid, or test
        set_path (str): path to a set of cleaned data where stats csv files are

        return params
        -------------
        df (pandas.DataFrame): re-formatted stats dataframe 
    '''

    ## Read the stats csv file from input path
    df = pandas.read_csv (set_path + dtype + '_stats.csv')
    ## Change the index to be station ID
    df.index = df.station_id
    df = df.drop (axis=1, columns='station_id').sort_index()
    ## Rename columns based on sub-folder name
    pieces = set_path.replace ('\\', '/').split ('/')
    ## Find the last non-empty piece i.e. valid folder name
    for piece in pieces[::-1]:
        if len (piece)>0:
            key = piece; break
    df.columns = [col + '_' + key for col in df.columns]

    return df

def get_stats (dtype, set1_path, set2_path=None, set3_path=None):

    ''' A function to read a specific dataset stats from all available sets.

        input params
        ------------
        dtype (str): either train, valid, or test
        set1_path (str): Path to processed data from set 1
        set2_path (str): If not None, path to processed data from set 2
        set3_path (str): If not None, path to processed data from set 3

        return params
        -------------
        stats (pandas.DataFrame): combined stats dataframe from csv file
    '''

    ## Start with set 1
    stats = get_a_stats (dtype, set1_path)

    ## If set2 is available, read and merge!
    if set2_path is not None:
        set2_stats = get_a_stats (dtype, set2_path)
        stats = pandas.merge (stats, set2_stats, how='outer',
                              right_index=True, left_index=True)

    ## Same for set 3
    if set3_path is not None:
        set3_stats = get_a_stats (dtype, set3_path)
        stats = pandas.merge (stats, set3_stats, how='outer',
                              right_index=True, left_index=True)

    return stats

def plot_bad_percentage (train_stats, valid_stats, test_stats):

    ''' A function to plot bad percentages.
            * Top plot is from training sets: # n_spikes / 200000 (the sample
              size Greg picked for AI model)
            * Middle plot is from validation sets: # n_spikes / # n_total_valid
            * Bottom plot is from testing sets: # n_spikes / # n_total_test
        Labeling are defined by the sub-folders of the sets. Colors and markers
        are defined as constants on top of the script.

        input params
        ------------
        train_stats (pandas.DataFrame): stats set from training sets
        valid_stats (pandas.DataFrame): stats set from validation sets
        test_stats (pandas.DataFrame): stats set from testing sets
    '''

    ## Define the name / label of the 3 sets
    labels = ['_'.join (col.split('_')[2:])
              for col in train_stats.columns 
              if 'n_spikes' in col and not 'percent' in col]

    ## Start plotting!
    h = plt.figure (figsize=(9, 9))
    gs = gridspec.GridSpec (3, 1, wspace=0.1)
    gs.update (bottom=0.15)

    ## Top plot: training n_spikes w.r.t. N_RANDOM_SAMPLES
    axis = h.add_subplot (gs[0])
    xvalues = numpy.arange (len (train_stats))
    for index, label in enumerate (labels):
        yvalues = train_stats['n_spikes_' + label].values / N_RANDOM_SAMPLES
        axis.scatter (xvalues, yvalues, marker=markers[index],
                      color=colors[index], s=25, alpha=0.7, label=label)
    ## Shade "good" threshold as green i.e. good stations
    axis.fill_between (xvalues, 0, PCT_THRESH, color='green', alpha=0.2)

    ##  Format x-axis
    axis.set_xlim ([min(xvalues)-1, max(xvalues)+1])
    axis.set_xticks (xvalues)
    axis.get_xaxis ().set_ticklabels ([])
    ##  Format y-axis
    axis.set_ylim ([0, 0.5])
    axis.tick_params (axis='y', labelsize=8)
    axis.set_ylabel ('# spikes in train / {0}'.format (N_RANDOM_SAMPLES), fontsize=8)       
    ##  Plot grid lines
    for ytick in axis.yaxis.get_majorticklocs():
        axis.axhline (y=ytick, color='gray', alpha=0.3, linestyle=':', linewidth=0.2)
    for xtick in axis.xaxis.get_majorticklocs():
        axis.axvline (x=xtick, color='gray', alpha=0.3, linestyle=':', linewidth=0.2)       

    ## Middle plot: validation n_spikes w.r.t. # validation sample
    axis = h.add_subplot (gs[1])
    xvalues = numpy.arange (len (valid_stats))
    for index, label in enumerate (labels):
        yvalues = valid_stats['n_spikes_' + label].values / valid_stats['n_total_' + label].values
        axis.scatter (xvalues, yvalues, marker=markers[index],
                      color=colors[index], s=25, alpha=0.7, label=label)
    ## Shade "good" threshold as green i.e. good stations
    axis.fill_between (xvalues, 0, PCT_THRESH, color='green', alpha=0.2)
    ##  Plot legend in the middle plot
    axis.legend (loc=2, fontsize=8) 

    ##  Format x-axis
    axis.set_xlim ([min(xvalues)-1, max(xvalues)+1])
    axis.set_xticks (xvalues)
    axis.get_xaxis ().set_ticklabels ([])
    axis.tick_params (axis='x', labelsize=8, labelrotation=90)
    ##  Format y-axis
    axis.set_ylim ([0, 0.4])
    axis.tick_params (axis='y', labelsize=8)
    axis.set_ylabel ('# spikes in valid / # total in valid', fontsize=8)       
    ##  Plot grid lines
    for ytick in axis.yaxis.get_majorticklocs():
        axis.axhline (y=ytick, color='gray', alpha=0.3, linestyle=':', linewidth=0.2)
    for xtick in axis.xaxis.get_majorticklocs():
        axis.axvline (x=xtick, color='gray', alpha=0.3, linestyle=':', linewidth=0.2)

    ## Bottom plot: test n_spikes w.r.t. # test sample
    axis = h.add_subplot (gs[2])
    xvalues = numpy.arange (len (test_stats))
    for index, label in enumerate (labels):
        yvalues = test_stats['n_spikes_' + label].values / test_stats['n_total_' + label].values
        axis.scatter (xvalues, yvalues, marker=markers[index],
                      color=colors[index], s=25, alpha=0.7, label=label)
    ## Shade "good" threshold as green i.e. good stations
    axis.fill_between (xvalues, 0, PCT_THRESH, color='green', alpha=0.2)

    ##  Format x-axis
    axis.set_xlim ([min(xvalues)-1, max(xvalues)+1])
    axis.set_xticks (xvalues)
    axis.set_xticklabels (test_stats.index)
    axis.tick_params (axis='x', labelsize=8, labelrotation=90)
    ##  Format y-axis
    axis.set_ylim ([0, 0.25])
    axis.tick_params (axis='y', labelsize=8)
    axis.set_ylabel ('# spikes in test / # total in test', fontsize=8)       
    ##  Plot grid lines
    for ytick in axis.yaxis.get_majorticklocs():
        axis.axhline (y=ytick, color='gray', alpha=0.3, linestyle=':', linewidth=0.2)
    for xtick in axis.xaxis.get_majorticklocs():
        axis.axvline (x=xtick, color='gray', alpha=0.3, linestyle=':', linewidth=0.2)

    ### Store plot as PDF
    plt.suptitle ('Spike percentage', fontsize=15)
    h.savefig (out_path + '/spike_percentages.pdf')
    plt.close ('all')
    return

###############################################
## Script begins here!
###############################################
if __name__ == '__main__':

    out_path, set1_path, set2_path, set3_path = get_parser ()

    ## Gather all stats dataframe
    train_stats = get_stats ('train', set1_path, set2_path=set2_path, set3_path=set3_path)
    valid_stats = get_stats ('valid', set1_path, set2_path=set2_path, set3_path=set3_path)
    test_stats  = get_stats ('test' , set1_path, set2_path=set2_path, set3_path=set3_path)

    ## Plot bad percentage
    plot_bad_percentage (train_stats, valid_stats, test_stats)
