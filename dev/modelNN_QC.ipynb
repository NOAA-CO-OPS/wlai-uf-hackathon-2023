{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to train and evaulate the NN QC model on all 50+ gauge locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import itertools\n",
    "import dill\n",
    "import pickle\n",
    "from scipy import io\n",
    "from scipy.signal import detrend as detrend\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import roc_auc_score as roc\n",
    "from sklearn.metrics import roc_curve\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#My helper functions\n",
    "from modelNN_functions import assessTrainTestData\n",
    "from modelNN_functions import plotConfusionMatrix\n",
    "from modelNN_functions import pandasToMat\n",
    "from modelNN_functions import BSS\n",
    "from modelNN_functions import loadCleanedData\n",
    "from modelNN_functions import resampleGoodPointsSetNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the NN model to be used for the QC (good or bad 1=good, 0=bad)\n",
    "\n",
    "def runNNmodel(featureTrain, targetTrain, featureTest, targetTest, predictFeatures, predictTargets, \n",
    "               predictStations, epochCount, batchCount, classThreshold, numFeatures):\n",
    "\n",
    "    \"\"\"\n",
    "    where: \n",
    "        featureTrain = features used in training the model\n",
    "        targetTrain = targets (0 or 1) used in training the model\n",
    "        featureTest = features used for validation\n",
    "        targetTest = targets (0 or 1) used for valdation\n",
    "        predictFeatures = the features for the model predictions\n",
    "        predictTargets = the targets for the model predictions\n",
    "        predictStations = the stations for the model predictions\n",
    "        epochCount = number of training cycles to use for NN model (10-20 seems reasonable)\n",
    "        batchCount = the batch size to use when training (somewhere from 32-256 seems reasonable)\n",
    "        classThreshold = the threshold from 0 to 1 used to classify a point as good or bad (default is 0.5)\n",
    "        numFeatures = the number of input features in the NN model\n",
    "    \"\"\"\n",
    "\n",
    "    #First define the keras NN framework to be used\n",
    "    # For a single-input model with 2 classes (binary classification):\n",
    "    model = Sequential()\n",
    "    #model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_dim=numFeatures))\n",
    "    model.add(Dense(32, activation='relu', input_dim=numFeatures))\n",
    "    model.add(Dropout(0.25))\n",
    "    #model.add(Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "    model.add(Dense(32, activation='relu', input_dim=numFeatures))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    #Set up the model checkpoint\n",
    "    # checkpoint\n",
    "    filepath=\"model_best.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    # Train the model, iterating on the data in batches of X # samples (somewhere between 32 - 256)\n",
    "    history = model.fit(featureTrain, targetTrain, epochs=epochCount, batch_size=batchCount, validation_data=(featureTest, targetTest), callbacks=callbacks_list)\n",
    "    \n",
    "    # Load the best performing model run\n",
    "    model = load_model('model_best.hdf5')\n",
    "    \n",
    "    #Evaulate the model\n",
    "    eval_model=model.evaluate(featureTest, targetTest)\n",
    "    eval_model\n",
    "    \n",
    "    #Plot the NN accuracy over each epoch\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    ax.legend(['Training','Validation'])\n",
    "    plt.show()\n",
    "    #fig.savefig('NN_modelTrainingHistory.png')\n",
    "    \n",
    "    #Generate predictions for the test period\n",
    "    modelPrediction = model.predict(featureTest, batch_size=32)\n",
    "\n",
    "    #And now use the threhsold to decide y or n\n",
    "    modelPredThresh=1*(modelPrediction >= classThreshold)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnfMatrix = confusion_matrix(targetTest, modelPredThresh)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    classNames = ['bad data point','good data point']\n",
    "    plotConfusionMatrix(cnfMatrix, classes=classNames,normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "    plt.show()\n",
    "    \n",
    "    #Generate model predictions for the validation period\n",
    "    modelPrediction = model.predict(predictFeatures, batch_size=32)\n",
    "    #Generate the y/n prediction given the above model threshold value (right now = 0.9)\n",
    "    modelPredThresh=1*(modelPrediction >= classThreshold)\n",
    "    \n",
    "    modelOut = pd.DataFrame()\n",
    "    modelOut['primary'] = predictFeatures['PRIMARY']\n",
    "    modelOut['modelPrediction']=modelPrediction\n",
    "    modelOut['InvModelPrediction']=1-modelPrediction\n",
    "    modelOut['goodPtsPrediction']=modelPredThresh\n",
    "    modelOut['target'] = predictTargets['TARGET']\n",
    "    modelOut['station'] = predictStations['STATION_ID']\n",
    "    \n",
    "    return modelOut, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the station list \n",
    "#Grab the station list by default from the directory of files\n",
    "dirList = os.listdir('/jupyter/userhomes/dusek/waterlevelAI/data/train')\n",
    "stationList = []\n",
    "for file in dirList:\n",
    "    if not file.startswith('.'):\n",
    "        stationList.append(file[:7])\n",
    "        \n",
    "#removing south beach for now - has some sort of issue\n",
    "#stationList.remove('9435380')\n",
    "\n",
    "#Create performance dataframe\n",
    "#Make the modelPerformanceDataFrame to store info about each station and error stats\n",
    "modelPerformance = pd.DataFrame()\n",
    "stationList.insert(0,'allStations')\n",
    "modelPerformance['stationList'] = stationList\n",
    "modelPerformance.set_index('stationList',inplace = True)\n",
    "\n",
    "modelPerformance['trainingPts'] = np.nan\n",
    "modelPerformance['trainingFractionBad'] = np.nan\n",
    "modelPerformance['valPts'] = np.nan\n",
    "modelPerformance['valFractionBad'] = np.nan\n",
    "\n",
    "modelPerformance['accuracy'] = np.nan\n",
    "modelPerformance['badPtAccuracy'] = np.nan\n",
    "modelPerformance['BSS'] = np.nan\n",
    "modelPerformance['AROC'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1612340\n",
      "Random under-sampling:\n",
      "1    199917\n",
      "0        83\n",
      "Name: TARGET, dtype: int64\n",
      "1617433\n",
      "Random under-sampling:\n",
      "1    177913\n",
      "0     22087\n",
      "Name: TARGET, dtype: int64\n",
      "8418150\n",
      "Random under-sampling:\n",
      "1    193788\n",
      "0      6212\n",
      "Name: TARGET, dtype: int64\n",
      "8443970\n",
      "Random under-sampling:\n",
      "1    179062\n",
      "0     20938\n",
      "Name: TARGET, dtype: int64\n",
      "8449130\n",
      "Random under-sampling:\n",
      "1    187810\n",
      "0     12190\n",
      "Name: TARGET, dtype: int64\n",
      "8452660\n",
      "Random under-sampling:\n",
      "1    164543\n",
      "0     35457\n",
      "Name: TARGET, dtype: int64\n",
      "8454049\n",
      "Random under-sampling:\n",
      "1    185530\n",
      "0     14470\n",
      "Name: TARGET, dtype: int64\n",
      "8461490\n",
      "Random under-sampling:\n",
      "1    172262\n",
      "0     27738\n",
      "Name: TARGET, dtype: int64\n",
      "8510560\n",
      "Random under-sampling:\n",
      "1    132397\n",
      "0     67603\n",
      "Name: TARGET, dtype: int64\n",
      "8534720\n",
      "Random under-sampling:\n",
      "1    193061\n",
      "0      6939\n",
      "Name: TARGET, dtype: int64\n",
      "8536110\n",
      "Random under-sampling:\n",
      "1    167557\n",
      "0     32443\n",
      "Name: TARGET, dtype: int64\n",
      "8557380\n",
      "Random under-sampling:\n",
      "1    183001\n",
      "0     16999\n",
      "Name: TARGET, dtype: int64\n",
      "8573364\n",
      "Random under-sampling:\n",
      "1    121564\n",
      "0     78436\n",
      "Name: TARGET, dtype: int64\n",
      "8574680\n",
      "Random under-sampling:\n",
      "1    186592\n",
      "0     13408\n",
      "Name: TARGET, dtype: int64\n",
      "8638863\n",
      "Random under-sampling:\n",
      "1    199271\n",
      "0       729\n",
      "Name: TARGET, dtype: int64\n",
      "8651370\n",
      "Random under-sampling:\n",
      "1    189444\n",
      "0     10556\n",
      "Name: TARGET, dtype: int64\n",
      "8658120\n",
      "Random under-sampling:\n",
      "1    194131\n",
      "0      5869\n",
      "Name: TARGET, dtype: int64\n",
      "8658163\n",
      "Random under-sampling:\n",
      "1    196344\n",
      "0      3656\n",
      "Name: TARGET, dtype: int64\n",
      "8665530\n",
      "Random under-sampling:\n",
      "1    199553\n",
      "0       447\n",
      "Name: TARGET, dtype: int64\n",
      "8670870\n",
      "Random under-sampling:\n",
      "1    199034\n",
      "0       966\n",
      "Name: TARGET, dtype: int64\n",
      "8720030\n",
      "Random under-sampling:\n",
      "1    189030\n",
      "0     10970\n",
      "Name: TARGET, dtype: int64\n",
      "8721604\n",
      "Random under-sampling:\n",
      "1    169142\n",
      "0     30858\n",
      "Name: TARGET, dtype: int64\n",
      "8723214\n",
      "Random under-sampling:\n",
      "1    184704\n",
      "0     15296\n",
      "Name: TARGET, dtype: int64\n",
      "8726520\n",
      "Random under-sampling:\n",
      "1    198586\n",
      "0      1414\n",
      "Name: TARGET, dtype: int64\n",
      "8726607\n",
      "Random under-sampling:\n",
      "1    191393\n",
      "0      8607\n",
      "Name: TARGET, dtype: int64\n",
      "8729108\n",
      "Random under-sampling:\n",
      "1    199350\n",
      "0       650\n",
      "Name: TARGET, dtype: int64\n",
      "8729840\n",
      "Random under-sampling:\n",
      "1    181799\n",
      "0     18201\n",
      "Name: TARGET, dtype: int64\n",
      "8735180\n",
      "Random under-sampling:\n",
      "1    197282\n",
      "0      2718\n",
      "Name: TARGET, dtype: int64\n",
      "8741533\n",
      "Random under-sampling:\n",
      "1    198171\n",
      "0      1829\n",
      "Name: TARGET, dtype: int64\n",
      "8767816\n",
      "Random under-sampling:\n",
      "1    163346\n",
      "0     36654\n",
      "Name: TARGET, dtype: int64\n",
      "8767961\n",
      "Random under-sampling:\n",
      "1    199668\n",
      "0       332\n",
      "Name: TARGET, dtype: int64\n",
      "8771341\n",
      "Random under-sampling:\n",
      "1    160113\n",
      "0     39887\n",
      "Name: TARGET, dtype: int64\n",
      "8771450\n",
      "Random under-sampling:\n",
      "1    194075\n",
      "0      5925\n",
      "Name: TARGET, dtype: int64\n",
      "8775870\n",
      "Random under-sampling:\n",
      "1    199077\n",
      "0       923\n",
      "Name: TARGET, dtype: int64\n",
      "8779770\n",
      "Random under-sampling:\n",
      "1    195113\n",
      "0      4887\n",
      "Name: TARGET, dtype: int64\n",
      "9410840\n",
      "Random under-sampling:\n",
      "1    199808\n",
      "0       192\n",
      "Name: TARGET, dtype: int64\n",
      "9411340\n",
      "Random under-sampling:\n",
      "1    186062\n",
      "0     13938\n",
      "Name: TARGET, dtype: int64\n",
      "9414290\n",
      "Random under-sampling:\n",
      "1    156521\n",
      "0     43479\n",
      "Name: TARGET, dtype: int64\n",
      "9414750\n",
      "Random under-sampling:\n",
      "1    199768\n",
      "0       232\n",
      "Name: TARGET, dtype: int64\n",
      "9418767\n",
      "Random under-sampling:\n",
      "1    198840\n",
      "0      1160\n",
      "Name: TARGET, dtype: int64\n",
      "9419750\n",
      "Random under-sampling:\n",
      "1    189864\n",
      "0     10136\n",
      "Name: TARGET, dtype: int64\n",
      "9432780\n",
      "Random under-sampling:\n",
      "1    192085\n",
      "0      7915\n",
      "Name: TARGET, dtype: int64\n",
      "9446484\n",
      "Random under-sampling:\n",
      "1    197563\n",
      "0      2437\n",
      "Name: TARGET, dtype: int64\n",
      "9447130\n",
      "Random under-sampling:\n",
      "1    199815\n",
      "0       185\n",
      "Name: TARGET, dtype: int64\n",
      "9459450\n",
      "Random under-sampling:\n",
      "1    198880\n",
      "0      1120\n",
      "Name: TARGET, dtype: int64\n",
      "9459881\n",
      "Random under-sampling:\n",
      "1    199313\n",
      "0       687\n",
      "Name: TARGET, dtype: int64\n",
      "9751381\n",
      "Random under-sampling:\n",
      "1    199900\n",
      "0       100\n",
      "Name: TARGET, dtype: int64\n",
      "9751639\n",
      "Random under-sampling:\n",
      "1    198274\n",
      "0      1726\n",
      "Name: TARGET, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainingPts</th>\n",
       "      <th>trainingFractionBad</th>\n",
       "      <th>valPts</th>\n",
       "      <th>valFractionBad</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>badPtAccuracy</th>\n",
       "      <th>BSS</th>\n",
       "      <th>AROC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stationList</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allStations</th>\n",
       "      <td>9600000.0</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>8123660.0</td>\n",
       "      <td>0.022440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612340</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>175160.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617433</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.110435</td>\n",
       "      <td>172421.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418150</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>175151.0</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443970</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.104690</td>\n",
       "      <td>166380.0</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449130</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.060950</td>\n",
       "      <td>173082.0</td>\n",
       "      <td>0.019349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452660</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.177285</td>\n",
       "      <td>170839.0</td>\n",
       "      <td>0.204151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8454049</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.072350</td>\n",
       "      <td>173273.0</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8461490</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.138690</td>\n",
       "      <td>174769.0</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510560</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.338015</td>\n",
       "      <td>164740.0</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8534720</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.034695</td>\n",
       "      <td>174506.0</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8536110</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.162215</td>\n",
       "      <td>175120.0</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8557380</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.084995</td>\n",
       "      <td>174599.0</td>\n",
       "      <td>0.009668</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573364</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.392180</td>\n",
       "      <td>174512.0</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574680</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.067040</td>\n",
       "      <td>174049.0</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638863</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>64932.0</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651370</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.052780</td>\n",
       "      <td>174604.0</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658120</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>175146.0</td>\n",
       "      <td>0.056576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658163</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>0.064202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8665530</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>174910.0</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670870</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>174783.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720030</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>174856.0</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721604</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.154290</td>\n",
       "      <td>152833.0</td>\n",
       "      <td>0.033062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723214</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.076480</td>\n",
       "      <td>172414.0</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726520</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>166782.0</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726607</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.043035</td>\n",
       "      <td>168903.0</td>\n",
       "      <td>0.015601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729108</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>175141.0</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729840</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.091005</td>\n",
       "      <td>173525.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735180</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>174906.0</td>\n",
       "      <td>0.015340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8741533</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>174969.0</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8767816</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.183270</td>\n",
       "      <td>173171.0</td>\n",
       "      <td>0.013351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8767961</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>175137.0</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771341</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.199435</td>\n",
       "      <td>175131.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771450</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.029625</td>\n",
       "      <td>175128.0</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8775870</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>171128.0</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8779770</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.024435</td>\n",
       "      <td>173655.0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9410840</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>174966.0</td>\n",
       "      <td>0.118943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411340</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.069690</td>\n",
       "      <td>175106.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414290</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.217395</td>\n",
       "      <td>174744.0</td>\n",
       "      <td>0.045873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414750</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>174674.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418767</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>174679.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419750</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>171435.0</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432780</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.039575</td>\n",
       "      <td>175101.0</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9446484</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>174845.0</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9447130</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>166190.0</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459450</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>174330.0</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459881</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>172552.0</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751381</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>173582.0</td>\n",
       "      <td>0.268254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751639</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>105901.0</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trainingPts  trainingFractionBad     valPts  valFractionBad  \\\n",
       "stationList                                                                \n",
       "allStations    9600000.0             0.066634  8123660.0        0.022440   \n",
       "1612340         200000.0             0.000415   175160.0        0.000023   \n",
       "1617433         200000.0             0.110435   172421.0        0.000006   \n",
       "8418150         200000.0             0.031060   175151.0        0.010488   \n",
       "8443970         200000.0             0.104690   166380.0        0.001695   \n",
       "8449130         200000.0             0.060950   173082.0        0.019349   \n",
       "8452660         200000.0             0.177285   170839.0        0.204151   \n",
       "8454049         200000.0             0.072350   173273.0        0.002920   \n",
       "8461490         200000.0             0.138690   174769.0        0.020358   \n",
       "8510560         200000.0             0.338015   164740.0        0.007327   \n",
       "8534720         200000.0             0.034695   174506.0        0.027426   \n",
       "8536110         200000.0             0.162215   175120.0        0.017668   \n",
       "8557380         200000.0             0.084995   174599.0        0.009668   \n",
       "8573364         200000.0             0.392180   174512.0        0.033998   \n",
       "8574680         200000.0             0.067040   174049.0        0.008630   \n",
       "8638863         200000.0             0.003645    64932.0        0.000909   \n",
       "8651370         200000.0             0.052780   174604.0        0.000200   \n",
       "8658120         200000.0             0.029345   175146.0        0.056576   \n",
       "8658163         200000.0             0.018280   174900.0        0.064202   \n",
       "8665530         200000.0             0.002235   174910.0        0.012223   \n",
       "8670870         200000.0             0.004830   174783.0        0.000046   \n",
       "8720030         200000.0             0.054850   174856.0        0.000063   \n",
       "8721604         200000.0             0.154290   152833.0        0.033062   \n",
       "8723214         200000.0             0.076480   172414.0        0.002100   \n",
       "8726520         200000.0             0.007070   166782.0        0.002596   \n",
       "8726607         200000.0             0.043035   168903.0        0.015601   \n",
       "8729108         200000.0             0.003250   175141.0        0.000691   \n",
       "8729840         200000.0             0.091005   173525.0        0.000023   \n",
       "8735180         200000.0             0.013590   174906.0        0.015340   \n",
       "8741533         200000.0             0.009145   174969.0        0.001920   \n",
       "8767816         200000.0             0.183270   173171.0        0.013351   \n",
       "8767961         200000.0             0.001660   175137.0        0.000103   \n",
       "8771341         200000.0             0.199435   175131.0        0.000046   \n",
       "8771450         200000.0             0.029625   175128.0        0.002827   \n",
       "8775870         200000.0             0.004615   171128.0        0.024853   \n",
       "8779770         200000.0             0.024435   173655.0        0.000069   \n",
       "9410840         200000.0             0.000960   174966.0        0.118943   \n",
       "9411340         200000.0             0.069690   175106.0        0.000011   \n",
       "9414290         200000.0             0.217395   174744.0        0.045873   \n",
       "9414750         200000.0             0.001160   174674.0        0.000011   \n",
       "9418767         200000.0             0.005800   174679.0        0.000006   \n",
       "9419750         200000.0             0.050680   171435.0        0.006405   \n",
       "9432780         200000.0             0.039575   175101.0        0.000148   \n",
       "9446484         200000.0             0.012185   174845.0        0.002454   \n",
       "9447130         200000.0             0.000925   166190.0        0.001763   \n",
       "9459450         200000.0             0.005600   174330.0        0.000218   \n",
       "9459881         200000.0             0.003435   172552.0        0.001495   \n",
       "9751381         200000.0             0.000500   173582.0        0.268254   \n",
       "9751639         200000.0             0.008630   105901.0        0.000198   \n",
       "\n",
       "             accuracy  badPtAccuracy  BSS  AROC  \n",
       "stationList                                      \n",
       "allStations       NaN            NaN  NaN   NaN  \n",
       "1612340           NaN            NaN  NaN   NaN  \n",
       "1617433           NaN            NaN  NaN   NaN  \n",
       "8418150           NaN            NaN  NaN   NaN  \n",
       "8443970           NaN            NaN  NaN   NaN  \n",
       "8449130           NaN            NaN  NaN   NaN  \n",
       "8452660           NaN            NaN  NaN   NaN  \n",
       "8454049           NaN            NaN  NaN   NaN  \n",
       "8461490           NaN            NaN  NaN   NaN  \n",
       "8510560           NaN            NaN  NaN   NaN  \n",
       "8534720           NaN            NaN  NaN   NaN  \n",
       "8536110           NaN            NaN  NaN   NaN  \n",
       "8557380           NaN            NaN  NaN   NaN  \n",
       "8573364           NaN            NaN  NaN   NaN  \n",
       "8574680           NaN            NaN  NaN   NaN  \n",
       "8638863           NaN            NaN  NaN   NaN  \n",
       "8651370           NaN            NaN  NaN   NaN  \n",
       "8658120           NaN            NaN  NaN   NaN  \n",
       "8658163           NaN            NaN  NaN   NaN  \n",
       "8665530           NaN            NaN  NaN   NaN  \n",
       "8670870           NaN            NaN  NaN   NaN  \n",
       "8720030           NaN            NaN  NaN   NaN  \n",
       "8721604           NaN            NaN  NaN   NaN  \n",
       "8723214           NaN            NaN  NaN   NaN  \n",
       "8726520           NaN            NaN  NaN   NaN  \n",
       "8726607           NaN            NaN  NaN   NaN  \n",
       "8729108           NaN            NaN  NaN   NaN  \n",
       "8729840           NaN            NaN  NaN   NaN  \n",
       "8735180           NaN            NaN  NaN   NaN  \n",
       "8741533           NaN            NaN  NaN   NaN  \n",
       "8767816           NaN            NaN  NaN   NaN  \n",
       "8767961           NaN            NaN  NaN   NaN  \n",
       "8771341           NaN            NaN  NaN   NaN  \n",
       "8771450           NaN            NaN  NaN   NaN  \n",
       "8775870           NaN            NaN  NaN   NaN  \n",
       "8779770           NaN            NaN  NaN   NaN  \n",
       "9410840           NaN            NaN  NaN   NaN  \n",
       "9411340           NaN            NaN  NaN   NaN  \n",
       "9414290           NaN            NaN  NaN   NaN  \n",
       "9414750           NaN            NaN  NaN   NaN  \n",
       "9418767           NaN            NaN  NaN   NaN  \n",
       "9419750           NaN            NaN  NaN   NaN  \n",
       "9432780           NaN            NaN  NaN   NaN  \n",
       "9446484           NaN            NaN  NaN   NaN  \n",
       "9447130           NaN            NaN  NaN   NaN  \n",
       "9459450           NaN            NaN  NaN   NaN  \n",
       "9459881           NaN            NaN  NaN   NaN  \n",
       "9751381           NaN            NaN  NaN   NaN  \n",
       "9751639           NaN            NaN  NaN   NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each station in my list of stations I want to include, load the training data and validation data.  \n",
    "# Also - resample to create data sets that are 10% bad data points and then cat together\n",
    "\n",
    "# For each station in list load the training and validation data, print the data counts, resample and cat together\n",
    "trainAll = pd.DataFrame()\n",
    "validateAll = pd.DataFrame()\n",
    "allTrainData=0\n",
    "allTrainBadData=0\n",
    "allValData=0\n",
    "allValBadData=0\n",
    "\n",
    "for station in stationList[1:]:\n",
    "    print(station)\n",
    "    #load the training data\n",
    "    trainIn = loadCleanedData(station, 'train', '/jupyter/userhomes/dusek/waterlevelAI/data')\n",
    "    validateIn = loadCleanedData(station, 'validation', '/jupyter/userhomes/dusek/waterlevelAI/data')\n",
    "    \n",
    "    #Remove all cases where PRIMARY_TRUE = 0, since then the primary data is missing, and we aren't assessing its quality\n",
    "    trainIn.drop(trainIn[trainIn['PRIMARY_TRUE'] == 0].index, inplace = True) \n",
    "    validateIn.drop(validateIn[validateIn['PRIMARY_TRUE'] == 0].index, inplace = True) \n",
    "    \n",
    "    #resample the training data\n",
    "    trainResample = resampleGoodPointsSetNum(200000, trainIn)    \n",
    "    \n",
    "    #assess the data counts\n",
    "    [totalTrainData,badTrainData] = assessTrainTestData(trainResample)\n",
    "    [totalValData,badValData] = assessTrainTestData(validateIn)\n",
    "    \n",
    "    #load into the dataframe\n",
    "    modelPerformance.at[station,'trainingPts'] = totalTrainData\n",
    "    modelPerformance.at[station,'trainingFractionBad'] = badTrainData/totalTrainData\n",
    "    modelPerformance.at[station,'valPts'] = totalValData\n",
    "    modelPerformance.at[station,'valFractionBad'] = badValData/totalValData\n",
    "    \n",
    "    #Cat into a single dataframe\n",
    "    trainAll = pd.concat([trainAll, trainResample])\n",
    "    validateAll = pd.concat([validateAll, validateIn])\n",
    "    \n",
    "    #Calculate the total data numbers\n",
    "    allTrainData= allTrainData + totalTrainData\n",
    "    allTrainBadData= allTrainBadData+badTrainData\n",
    "    allValData= allValData + totalValData\n",
    "    allValBadData= allValBadData + badValData\n",
    "\n",
    "\n",
    "#Add the total data numbers to the dataframe\n",
    "modelPerformance.at['allStations','trainingPts'] = allTrainData\n",
    "modelPerformance.at['allStations','trainingFractionBad'] = allTrainBadData/allTrainData\n",
    "modelPerformance.at['allStations','valPts'] = allValData\n",
    "modelPerformance.at['allStations','valFractionBad'] = allValBadData/allValData\n",
    "\n",
    "# Shuffle data\n",
    "trainRand=shuffle(trainAll)\n",
    "validateRand=shuffle(validateAll)\n",
    "\n",
    "\n",
    "modelPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now set up the model inputs \n",
    "\n",
    "featureNames = ['PRIMARY','PRIMARY_SIGMA','PRIMARY_SIGMA_TRUE','PRIMARY_RESIDUAL','BACKUP','BACKUP_TRUE','PREDICTION',]\n",
    "\n",
    "featureTrain=trainRand.loc[:, featureNames]\n",
    "featureVal=validateRand.loc[:, featureNames]\n",
    "\n",
    "targetTrain=trainRand.loc[:,['TARGET']]\n",
    "targetVal=validateRand.loc[:,['TARGET']]\n",
    "\n",
    "\n",
    "predictFeatures = validateAll.loc[:, featureNames]\n",
    "predictTargets = validateAll.loc[:,['TARGET']]\n",
    "predictStations = validateAll.loc[:,['STATION_ID']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,345\n",
      "Trainable params: 1,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 9600000 samples, validate on 8123660 samples\n",
      "Epoch 1/30\n",
      "9600000/9600000 [==============================] - 108s 11us/step - loss: 0.1421 - accuracy: 0.9603 - val_loss: 0.0836 - val_accuracy: 0.9844\n",
      "Epoch 2/30\n",
      "9600000/9600000 [==============================] - 106s 11us/step - loss: 0.1275 - accuracy: 0.9633 - val_loss: 0.0866 - val_accuracy: 0.9846\n",
      "Epoch 3/30\n",
      "9600000/9600000 [==============================] - 106s 11us/step - loss: 0.1253 - accuracy: 0.9639 - val_loss: 0.0877 - val_accuracy: 0.9844\n",
      "Epoch 4/30\n",
      "9600000/9600000 [==============================] - 106s 11us/step - loss: 0.1238 - accuracy: 0.9642 - val_loss: 0.0907 - val_accuracy: 0.9845\n",
      "Epoch 5/30\n",
      "9600000/9600000 [==============================] - 106s 11us/step - loss: 0.1225 - accuracy: 0.9645 - val_loss: 0.0924 - val_accuracy: 0.9852\n",
      "Epoch 6/30\n",
      "9600000/9600000 [==============================] - 84s 9us/step - loss: 0.1217 - accuracy: 0.9648 - val_loss: 0.0913 - val_accuracy: 0.9845\n",
      "Epoch 7/30\n",
      "9600000/9600000 [==============================] - 66s 7us/step - loss: 0.1209 - accuracy: 0.9649 - val_loss: 0.0913 - val_accuracy: 0.9850\n",
      "Epoch 8/30\n",
      "9600000/9600000 [==============================] - 64s 7us/step - loss: 0.1204 - accuracy: 0.9650 - val_loss: 0.0948 - val_accuracy: 0.9847\n",
      "Epoch 9/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1200 - accuracy: 0.9652 - val_loss: 0.0931 - val_accuracy: 0.9836\n",
      "Epoch 10/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1195 - accuracy: 0.9653 - val_loss: 0.0930 - val_accuracy: 0.9846\n",
      "Epoch 11/30\n",
      "9600000/9600000 [==============================] - 64s 7us/step - loss: 0.1193 - accuracy: 0.9653 - val_loss: 0.0946 - val_accuracy: 0.9839\n",
      "Epoch 12/30\n",
      "9600000/9600000 [==============================] - 64s 7us/step - loss: 0.1189 - accuracy: 0.9654 - val_loss: 0.0887 - val_accuracy: 0.9842\n",
      "Epoch 13/30\n",
      "9600000/9600000 [==============================] - 69s 7us/step - loss: 0.1192 - accuracy: 0.9654 - val_loss: 0.0880 - val_accuracy: 0.9842\n",
      "Epoch 14/30\n",
      "9600000/9600000 [==============================] - 69s 7us/step - loss: 0.1184 - accuracy: 0.9655 - val_loss: 0.0852 - val_accuracy: 0.9839\n",
      "Epoch 15/30\n",
      "9600000/9600000 [==============================] - 70s 7us/step - loss: 0.1181 - accuracy: 0.9655 - val_loss: 0.0886 - val_accuracy: 0.9845\n",
      "Epoch 16/30\n",
      "9600000/9600000 [==============================] - 69s 7us/step - loss: 0.1178 - accuracy: 0.9656 - val_loss: 0.0933 - val_accuracy: 0.9842\n",
      "Epoch 17/30\n",
      "9600000/9600000 [==============================] - 70s 7us/step - loss: 0.1180 - accuracy: 0.9657 - val_loss: 0.0955 - val_accuracy: 0.9842\n",
      "Epoch 18/30\n",
      "9600000/9600000 [==============================] - 70s 7us/step - loss: 0.1176 - accuracy: 0.9657 - val_loss: 0.0901 - val_accuracy: 0.9850\n",
      "Epoch 19/30\n",
      "9600000/9600000 [==============================] - 64s 7us/step - loss: 0.1178 - accuracy: 0.9657 - val_loss: 0.0913 - val_accuracy: 0.9843\n",
      "Epoch 20/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1175 - accuracy: 0.9657 - val_loss: 0.0940 - val_accuracy: 0.9838\n",
      "Epoch 21/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1174 - accuracy: 0.9657 - val_loss: 0.0928 - val_accuracy: 0.9845\n",
      "Epoch 22/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1172 - accuracy: 0.9658 - val_loss: 0.0931 - val_accuracy: 0.9841\n",
      "Epoch 23/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1171 - accuracy: 0.9658 - val_loss: 0.0911 - val_accuracy: 0.9839\n",
      "Epoch 24/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1171 - accuracy: 0.9658 - val_loss: 0.0905 - val_accuracy: 0.9847\n",
      "Epoch 25/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1172 - accuracy: 0.9658 - val_loss: 0.0899 - val_accuracy: 0.9850\n",
      "Epoch 26/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1168 - accuracy: 0.9658 - val_loss: 0.0903 - val_accuracy: 0.9840\n",
      "Epoch 27/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1167 - accuracy: 0.9658 - val_loss: 0.0914 - val_accuracy: 0.9844\n",
      "Epoch 28/30\n",
      "9600000/9600000 [==============================] - 63s 7us/step - loss: 0.1168 - accuracy: 0.9658 - val_loss: 0.0874 - val_accuracy: 0.9847\n",
      "Epoch 29/30\n",
      "9600000/9600000 [==============================] - 70s 7us/step - loss: 0.1168 - accuracy: 0.9659 - val_loss: 0.0905 - val_accuracy: 0.9839\n",
      "Epoch 30/30\n",
      "9600000/9600000 [==============================] - 70s 7us/step - loss: 0.1166 - accuracy: 0.9658 - val_loss: 0.0920 - val_accuracy: 0.9845\n",
      "8123660/8123660 [==============================] - 100s 12us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.39 0.61]\n",
      " [0.   1.  ]]\n",
      "total accuracy = 0.9844\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NN approach - 10% no - Simple model - with 32x2 and 0.25% dropout\n",
    "\n",
    "epochCount = 30\n",
    "batchCount = 256\n",
    "classThreshold = .50\n",
    "numFeatures = len(featureNames)\n",
    "modelOut_simple, model_simple = runNNmodel(featureTrain, targetTrain, featureVal, targetVal, predictFeatures, predictTargets,\n",
    "               predictStations, epochCount, batchCount, classThreshold, numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trainingPts</th>\n",
       "      <th>trainingFractionBad</th>\n",
       "      <th>valPts</th>\n",
       "      <th>valFractionBad</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>badPtAccuracy</th>\n",
       "      <th>BSS</th>\n",
       "      <th>AROC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stationList</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allStations</th>\n",
       "      <td>9600000.0</td>\n",
       "      <td>0.066634</td>\n",
       "      <td>8123660.0</td>\n",
       "      <td>0.022440</td>\n",
       "      <td>0.984415</td>\n",
       "      <td>0.392980</td>\n",
       "      <td>0.346773</td>\n",
       "      <td>0.703981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612340</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>175160.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952742</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617433</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.110435</td>\n",
       "      <td>172421.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953903</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418150</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.031060</td>\n",
       "      <td>175151.0</td>\n",
       "      <td>0.010488</td>\n",
       "      <td>0.998253</td>\n",
       "      <td>0.843223</td>\n",
       "      <td>0.887159</td>\n",
       "      <td>0.989764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443970</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.104690</td>\n",
       "      <td>166380.0</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.999297</td>\n",
       "      <td>0.780142</td>\n",
       "      <td>0.792695</td>\n",
       "      <td>0.897282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8449130</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.060950</td>\n",
       "      <td>173082.0</td>\n",
       "      <td>0.019349</td>\n",
       "      <td>0.995979</td>\n",
       "      <td>0.834876</td>\n",
       "      <td>0.657450</td>\n",
       "      <td>0.984724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8452660</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.177285</td>\n",
       "      <td>170839.0</td>\n",
       "      <td>0.204151</td>\n",
       "      <td>0.921136</td>\n",
       "      <td>0.614990</td>\n",
       "      <td>0.586796</td>\n",
       "      <td>0.883470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8454049</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.072350</td>\n",
       "      <td>173273.0</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.999544</td>\n",
       "      <td>0.893281</td>\n",
       "      <td>0.849161</td>\n",
       "      <td>0.986656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8461490</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.138690</td>\n",
       "      <td>174769.0</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.994141</td>\n",
       "      <td>0.722597</td>\n",
       "      <td>0.771083</td>\n",
       "      <td>0.986707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510560</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.338015</td>\n",
       "      <td>164740.0</td>\n",
       "      <td>0.007327</td>\n",
       "      <td>0.991987</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.456569</td>\n",
       "      <td>0.972229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8534720</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.034695</td>\n",
       "      <td>174506.0</td>\n",
       "      <td>0.027426</td>\n",
       "      <td>0.973101</td>\n",
       "      <td>0.340159</td>\n",
       "      <td>0.080611</td>\n",
       "      <td>0.873474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8536110</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.162215</td>\n",
       "      <td>175120.0</td>\n",
       "      <td>0.017668</td>\n",
       "      <td>0.997162</td>\n",
       "      <td>0.839367</td>\n",
       "      <td>0.893847</td>\n",
       "      <td>0.997414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8557380</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.084995</td>\n",
       "      <td>174599.0</td>\n",
       "      <td>0.009668</td>\n",
       "      <td>0.998047</td>\n",
       "      <td>0.824645</td>\n",
       "      <td>0.864350</td>\n",
       "      <td>0.998240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8573364</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.392180</td>\n",
       "      <td>174512.0</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.984173</td>\n",
       "      <td>0.617057</td>\n",
       "      <td>0.466751</td>\n",
       "      <td>0.955552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8574680</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.067040</td>\n",
       "      <td>174049.0</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.993169</td>\n",
       "      <td>0.380826</td>\n",
       "      <td>0.076343</td>\n",
       "      <td>0.943763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8638863</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>64932.0</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.999985</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.631181</td>\n",
       "      <td>0.983877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8651370</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.052780</td>\n",
       "      <td>174604.0</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.634535</td>\n",
       "      <td>0.976765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658120</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.029345</td>\n",
       "      <td>175146.0</td>\n",
       "      <td>0.056576</td>\n",
       "      <td>0.969220</td>\n",
       "      <td>0.652437</td>\n",
       "      <td>0.483638</td>\n",
       "      <td>0.844350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8658163</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.018280</td>\n",
       "      <td>174900.0</td>\n",
       "      <td>0.064202</td>\n",
       "      <td>0.999943</td>\n",
       "      <td>0.999109</td>\n",
       "      <td>0.993554</td>\n",
       "      <td>0.999601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8665530</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>174910.0</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>0.999125</td>\n",
       "      <td>0.966324</td>\n",
       "      <td>0.919436</td>\n",
       "      <td>0.984817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670870</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>174783.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.995652</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.332204</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8720030</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>174856.0</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.998936</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.350203</td>\n",
       "      <td>0.862133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8721604</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.154290</td>\n",
       "      <td>152833.0</td>\n",
       "      <td>0.033062</td>\n",
       "      <td>0.986017</td>\n",
       "      <td>0.577479</td>\n",
       "      <td>0.581021</td>\n",
       "      <td>0.801983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8723214</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.076480</td>\n",
       "      <td>172414.0</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.997924</td>\n",
       "      <td>0.969613</td>\n",
       "      <td>0.018391</td>\n",
       "      <td>0.989871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726520</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>166782.0</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.996534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380181</td>\n",
       "      <td>0.622416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8726607</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.043035</td>\n",
       "      <td>168903.0</td>\n",
       "      <td>0.015601</td>\n",
       "      <td>0.994062</td>\n",
       "      <td>0.816319</td>\n",
       "      <td>0.604668</td>\n",
       "      <td>0.997914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729108</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.003250</td>\n",
       "      <td>175141.0</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.997185</td>\n",
       "      <td>0.628099</td>\n",
       "      <td>0.338784</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8729840</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.091005</td>\n",
       "      <td>173525.0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.991598</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.414835</td>\n",
       "      <td>0.897376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8735180</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.013590</td>\n",
       "      <td>174906.0</td>\n",
       "      <td>0.015340</td>\n",
       "      <td>0.987279</td>\n",
       "      <td>0.187477</td>\n",
       "      <td>0.190955</td>\n",
       "      <td>0.758746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8741533</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.009145</td>\n",
       "      <td>174969.0</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.999697</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.283773</td>\n",
       "      <td>0.985728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8767816</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.183270</td>\n",
       "      <td>173171.0</td>\n",
       "      <td>0.013351</td>\n",
       "      <td>0.971947</td>\n",
       "      <td>0.938149</td>\n",
       "      <td>-0.397922</td>\n",
       "      <td>0.986215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8767961</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>175137.0</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.103397</td>\n",
       "      <td>0.996264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771341</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.199435</td>\n",
       "      <td>175131.0</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.999823</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.117984</td>\n",
       "      <td>0.413194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771450</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.029625</td>\n",
       "      <td>175128.0</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.998527</td>\n",
       "      <td>0.478788</td>\n",
       "      <td>0.423956</td>\n",
       "      <td>0.926373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8775870</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>171128.0</td>\n",
       "      <td>0.024853</td>\n",
       "      <td>0.978852</td>\n",
       "      <td>0.374794</td>\n",
       "      <td>0.158003</td>\n",
       "      <td>0.823119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8779770</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.024435</td>\n",
       "      <td>173655.0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.996297</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>-0.196681</td>\n",
       "      <td>0.730426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9410840</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>174966.0</td>\n",
       "      <td>0.118943</td>\n",
       "      <td>0.881068</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>-0.080595</td>\n",
       "      <td>0.222471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411340</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.069690</td>\n",
       "      <td>175106.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.967729</td>\n",
       "      <td>0.997836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414290</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.217395</td>\n",
       "      <td>174744.0</td>\n",
       "      <td>0.045873</td>\n",
       "      <td>0.954127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010594</td>\n",
       "      <td>0.702665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9414750</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>174674.0</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954406</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9418767</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>174679.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965169</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9419750</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>171435.0</td>\n",
       "      <td>0.006405</td>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.974499</td>\n",
       "      <td>0.966252</td>\n",
       "      <td>0.995691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432780</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.039575</td>\n",
       "      <td>175101.0</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.964180</td>\n",
       "      <td>0.944825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9446484</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.012185</td>\n",
       "      <td>174845.0</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.998999</td>\n",
       "      <td>0.627040</td>\n",
       "      <td>0.806099</td>\n",
       "      <td>0.994199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9447130</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>166190.0</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.999862</td>\n",
       "      <td>0.921502</td>\n",
       "      <td>0.959849</td>\n",
       "      <td>0.996151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459450</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>174330.0</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.885094</td>\n",
       "      <td>0.929557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9459881</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.003435</td>\n",
       "      <td>172552.0</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.998650</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>0.736999</td>\n",
       "      <td>0.752159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751381</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>173582.0</td>\n",
       "      <td>0.268254</td>\n",
       "      <td>0.732305</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>-0.030270</td>\n",
       "      <td>0.476058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9751639</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>105901.0</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.941674</td>\n",
       "      <td>0.930341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             trainingPts  trainingFractionBad     valPts  valFractionBad  \\\n",
       "stationList                                                                \n",
       "allStations    9600000.0             0.066634  8123660.0        0.022440   \n",
       "1612340         200000.0             0.000415   175160.0        0.000023   \n",
       "1617433         200000.0             0.110435   172421.0        0.000006   \n",
       "8418150         200000.0             0.031060   175151.0        0.010488   \n",
       "8443970         200000.0             0.104690   166380.0        0.001695   \n",
       "8449130         200000.0             0.060950   173082.0        0.019349   \n",
       "8452660         200000.0             0.177285   170839.0        0.204151   \n",
       "8454049         200000.0             0.072350   173273.0        0.002920   \n",
       "8461490         200000.0             0.138690   174769.0        0.020358   \n",
       "8510560         200000.0             0.338015   164740.0        0.007327   \n",
       "8534720         200000.0             0.034695   174506.0        0.027426   \n",
       "8536110         200000.0             0.162215   175120.0        0.017668   \n",
       "8557380         200000.0             0.084995   174599.0        0.009668   \n",
       "8573364         200000.0             0.392180   174512.0        0.033998   \n",
       "8574680         200000.0             0.067040   174049.0        0.008630   \n",
       "8638863         200000.0             0.003645    64932.0        0.000909   \n",
       "8651370         200000.0             0.052780   174604.0        0.000200   \n",
       "8658120         200000.0             0.029345   175146.0        0.056576   \n",
       "8658163         200000.0             0.018280   174900.0        0.064202   \n",
       "8665530         200000.0             0.002235   174910.0        0.012223   \n",
       "8670870         200000.0             0.004830   174783.0        0.000046   \n",
       "8720030         200000.0             0.054850   174856.0        0.000063   \n",
       "8721604         200000.0             0.154290   152833.0        0.033062   \n",
       "8723214         200000.0             0.076480   172414.0        0.002100   \n",
       "8726520         200000.0             0.007070   166782.0        0.002596   \n",
       "8726607         200000.0             0.043035   168903.0        0.015601   \n",
       "8729108         200000.0             0.003250   175141.0        0.000691   \n",
       "8729840         200000.0             0.091005   173525.0        0.000023   \n",
       "8735180         200000.0             0.013590   174906.0        0.015340   \n",
       "8741533         200000.0             0.009145   174969.0        0.001920   \n",
       "8767816         200000.0             0.183270   173171.0        0.013351   \n",
       "8767961         200000.0             0.001660   175137.0        0.000103   \n",
       "8771341         200000.0             0.199435   175131.0        0.000046   \n",
       "8771450         200000.0             0.029625   175128.0        0.002827   \n",
       "8775870         200000.0             0.004615   171128.0        0.024853   \n",
       "8779770         200000.0             0.024435   173655.0        0.000069   \n",
       "9410840         200000.0             0.000960   174966.0        0.118943   \n",
       "9411340         200000.0             0.069690   175106.0        0.000011   \n",
       "9414290         200000.0             0.217395   174744.0        0.045873   \n",
       "9414750         200000.0             0.001160   174674.0        0.000011   \n",
       "9418767         200000.0             0.005800   174679.0        0.000006   \n",
       "9419750         200000.0             0.050680   171435.0        0.006405   \n",
       "9432780         200000.0             0.039575   175101.0        0.000148   \n",
       "9446484         200000.0             0.012185   174845.0        0.002454   \n",
       "9447130         200000.0             0.000925   166190.0        0.001763   \n",
       "9459450         200000.0             0.005600   174330.0        0.000218   \n",
       "9459881         200000.0             0.003435   172552.0        0.001495   \n",
       "9751381         200000.0             0.000500   173582.0        0.268254   \n",
       "9751639         200000.0             0.008630   105901.0        0.000198   \n",
       "\n",
       "             accuracy  badPtAccuracy       BSS      AROC  \n",
       "stationList                                               \n",
       "allStations  0.984415       0.392980  0.346773  0.703981  \n",
       "1612340      1.000000       1.000000  0.952742  1.000000  \n",
       "1617433      1.000000       1.000000  0.953903  1.000000  \n",
       "8418150      0.998253       0.843223  0.887159  0.989764  \n",
       "8443970      0.999297       0.780142  0.792695  0.897282  \n",
       "8449130      0.995979       0.834876  0.657450  0.984724  \n",
       "8452660      0.921136       0.614990  0.586796  0.883470  \n",
       "8454049      0.999544       0.893281  0.849161  0.986656  \n",
       "8461490      0.994141       0.722597  0.771083  0.986707  \n",
       "8510560      0.991987       0.637117  0.456569  0.972229  \n",
       "8534720      0.973101       0.340159  0.080611  0.873474  \n",
       "8536110      0.997162       0.839367  0.893847  0.997414  \n",
       "8557380      0.998047       0.824645  0.864350  0.998240  \n",
       "8573364      0.984173       0.617057  0.466751  0.955552  \n",
       "8574680      0.993169       0.380826  0.076343  0.943763  \n",
       "8638863      0.999985       0.983051  0.631181  0.983877  \n",
       "8651370      0.999427       0.771429  0.634535  0.976765  \n",
       "8658120      0.969220       0.652437  0.483638  0.844350  \n",
       "8658163      0.999943       0.999109  0.993554  0.999601  \n",
       "8665530      0.999125       0.966324  0.919436  0.984817  \n",
       "8670870      0.995652       1.000000 -0.332204  1.000000  \n",
       "8720030      0.998936       0.545455  0.350203  0.862133  \n",
       "8721604      0.986017       0.577479  0.581021  0.801983  \n",
       "8723214      0.997924       0.969613  0.018391  0.989871  \n",
       "8726520      0.996534       0.000000  0.380181  0.622416  \n",
       "8726607      0.994062       0.816319  0.604668  0.997914  \n",
       "8729108      0.997185       0.628099  0.338784  0.955300  \n",
       "8729840      0.991598       0.500000 -1.414835  0.897376  \n",
       "8735180      0.987279       0.187477  0.190955  0.758746  \n",
       "8741533      0.999697       0.892857  0.283773  0.985728  \n",
       "8767816      0.971947       0.938149 -0.397922  0.986215  \n",
       "8767961      0.999949       0.944444  0.103397  0.996264  \n",
       "8771341      0.999823       0.125000  0.117984  0.413194  \n",
       "8771450      0.998527       0.478788  0.423956  0.926373  \n",
       "8775870      0.978852       0.374794  0.158003  0.823119  \n",
       "8779770      0.996297       0.416667 -0.196681  0.730426  \n",
       "9410840      0.881068       0.000096 -0.080595  0.222471  \n",
       "9411340      0.999989       0.000000  0.967729  0.997836  \n",
       "9414290      0.954127       0.000000 -0.010594  0.702665  \n",
       "9414750      1.000000       1.000000  0.954406  1.000000  \n",
       "9418767      0.999977       1.000000  0.965169  1.000000  \n",
       "9419750      0.999837       0.974499  0.966252  0.995691  \n",
       "9432780      0.999989       0.923077  0.964180  0.944825  \n",
       "9446484      0.998999       0.627040  0.806099  0.994199  \n",
       "9447130      0.999862       0.921502  0.959849  0.996151  \n",
       "9459450      0.999920       0.789474  0.885094  0.929557  \n",
       "9459881      0.998650       0.120155  0.736999  0.752159  \n",
       "9751381      0.732305       0.002105 -0.030270  0.476058  \n",
       "9751639      0.999877       0.380952  0.941674  0.930341  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The climatology comparison for the BSS will be accross all stations no matter what\n",
    "targetClimIn = targetTrain.to_numpy()\n",
    "\n",
    "#Loop through first all stations and then by station to calculate error stats\n",
    "for stationNum in modelPerformance.index:\n",
    "      \n",
    "    if stationNum == 'allStations':  \n",
    "        #define variables\n",
    "        goodPtsPredIn = modelOut_simple['goodPtsPrediction']\n",
    "        obsTargetIn = modelOut_simple['target'].to_numpy()\n",
    "        onlyBadPoints = modelOut_simple[modelOut_simple['target'] == 0]\n",
    "        goodPtsPredOnlyBadIn = onlyBadPoints['goodPtsPrediction']\n",
    "        obsTargetOnlyBadIn = onlyBadPoints['target']\n",
    "        modPredIn = modelOut_simple['modelPrediction'].to_numpy()   \n",
    "    else:\n",
    "        #define station specific variables\n",
    "        goodPtsPredIn = modelOut_simple[modelOut_simple['station']==int(stationNum)]['goodPtsPrediction']\n",
    "        obsTargetIn = modelOut_simple[modelOut_simple['station']==int(stationNum)]['target'].to_numpy()\n",
    "        onlyBadPoints = modelOut_simple[modelOut_simple['target'] == 0]\n",
    "        goodPtsPredOnlyBadIn = onlyBadPoints[onlyBadPoints['station']==int(stationNum)]['goodPtsPrediction']\n",
    "        obsTargetOnlyBadIn = onlyBadPoints[onlyBadPoints['station']==int(stationNum)]['target']\n",
    "        modPredIn =modelOut_simple[modelOut_simple['station']==int(stationNum)]['modelPrediction'].to_numpy()   \n",
    "        \n",
    "    \n",
    "    #Calculate the values\n",
    "    modelPerformance.at[stationNum,'accuracy'] = 1 - np.sum(np.abs(goodPtsPredIn - obsTargetIn))/len(goodPtsPredIn) \n",
    "\n",
    "    #Bad Pt Accuracy\n",
    "    modelPerformance.at[stationNum,'badPtAccuracy'] = 1 - np.sum(np.abs(goodPtsPredOnlyBadIn - obsTargetOnlyBadIn  ))/len(obsTargetOnlyBadIn) \n",
    "\n",
    "    #BSS\n",
    "    modelPerformance.at[stationNum,'BSS'] = BSS(targetClimIn, modPredIn, obsTargetIn)\n",
    "\n",
    "    #AROC\n",
    "    modelPerformance.at[stationNum,'AROC'] = roc(obsTargetIn,np.reshape(modPredIn,-1))\n",
    "\n",
    "        \n",
    "modelPerformance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wlai",
   "language": "python",
   "name": "wlai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
